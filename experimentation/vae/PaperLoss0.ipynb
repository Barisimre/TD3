{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "S_LAYER_SIZE = 10\n",
    "LATENT_SIZE = 3\n",
    "C = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 11])\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the file\n",
    "\n",
    "saved = np.load(\"testing.npy\", allow_pickle=True)\n",
    "\n",
    "# Make a trainging and testing batch\n",
    "train_data = torch.Tensor(saved[:int(len(saved)*0.5)])\n",
    "test_data = torch.Tensor(saved[int(len(saved)*0.5):])\n",
    "print(train_data.shape)\n",
    "INPUT_SIZE = len(test_data[0])\n",
    "INPUT_SIZE\n",
    "\n",
    "torch.set_printoptions(linewidth=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.l1 = nn.Linear(INPUT_SIZE, S_LAYER_SIZE)\n",
    "        self.l2a = nn.Linear(S_LAYER_SIZE, LATENT_SIZE)\n",
    "        self.l2b = nn.Linear(S_LAYER_SIZE, LATENT_SIZE)\n",
    "        \n",
    "        # Decoder\n",
    "        self.l3 = nn.Linear(LATENT_SIZE, S_LAYER_SIZE)\n",
    "        self.l4 = nn.Linear(S_LAYER_SIZE, INPUT_SIZE)\n",
    "            \n",
    "    # Run some data through the encoder\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "\n",
    "        # return the mu and the sigma\n",
    "        return self.l2a(x), self.l2b(x)\n",
    "    \n",
    "    # The reparameterization trick, taken from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        std = torch.exp(0.5*sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.relu(self.l3(x))\n",
    "\n",
    "        return torch.sigmoid(self.l4(x)) # sigmoid vs tanh\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: check the shape of x to be sure we have the right input\n",
    "        mu, sigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        # The loss function needs the mu and the sigma so just return them here\n",
    "        return self.decode(z), mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "# Taken from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "def loss_function(recon_x, x, mu, sigma):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())\n",
    "    return BCE + C*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e_count, model, optimizer):\n",
    "    data = train_data\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(0, len(data), BATCH_SIZE):\n",
    "        batch = data[i:i+BATCH_SIZE].to(device)\n",
    "        model.zero_grad()\n",
    "        recons, mu, sigma = model(batch)\n",
    "        loss = loss_function(recons, batch, mu, sigma)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch: {e_count}, Loss: {train_loss/len(data)}\")        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use CPU\n",
    "def test(e_count, model, optimizer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i in range(0, len(test_data), BATCH_SIZE):\n",
    "        batch = test_data[i:i+BATCH_SIZE].to(device)\n",
    "        recons, mu, sigma = model(batch)\n",
    "        loss = loss_function(recons, batch, mu, sigma)\n",
    "        test_loss += loss.item()\n",
    "    print(f\"TEST Epoch: {e_count}, Loss: {test_loss/len(test_data)}\") \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[torch.randperm(train_data.size()[0])]\n",
    "test_data=test_data[torch.randperm(test_data.size()[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae1 = VAE().to(device)\n",
    "opt1 = optim.Adam(vae1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 7.025108603515625\n",
      "Epoch: 1, Loss: 6.508820881347656\n",
      "Epoch: 2, Loss: 6.3603964331054685\n",
      "Epoch: 3, Loss: 6.2459430078125\n",
      "Epoch: 4, Loss: 6.235655709838867\n",
      "Epoch: 5, Loss: 6.234114617919922\n",
      "Epoch: 6, Loss: 6.233204861450195\n",
      "Epoch: 7, Loss: 6.232114788818359\n",
      "Epoch: 8, Loss: 6.230481822509765\n",
      "Epoch: 9, Loss: 6.227821557006836\n",
      "Epoch: 10, Loss: 6.22339450378418\n",
      "Epoch: 11, Loss: 6.21684160949707\n",
      "Epoch: 12, Loss: 6.211253404541016\n",
      "Epoch: 13, Loss: 6.208372300415039\n",
      "Epoch: 14, Loss: 6.206864089355469\n",
      "Epoch: 15, Loss: 6.205948841552734\n",
      "Epoch: 16, Loss: 6.205360250244141\n",
      "Epoch: 17, Loss: 6.204981746826172\n",
      "Epoch: 18, Loss: 6.2047454333496095\n",
      "Epoch: 19, Loss: 6.204602202758789\n",
      "Epoch: 20, Loss: 6.204530908813476\n",
      "Epoch: 21, Loss: 6.204494773559571\n",
      "Epoch: 22, Loss: 6.204439888305664\n",
      "Epoch: 23, Loss: 6.203620073852539\n",
      "Epoch: 24, Loss: 6.20306043762207\n",
      "Epoch: 25, Loss: 6.202915985107422\n",
      "Epoch: 26, Loss: 6.202832570800782\n",
      "Epoch: 27, Loss: 6.20277548461914\n",
      "Epoch: 28, Loss: 6.202734463500977\n",
      "Epoch: 29, Loss: 6.202701943359375\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    train(i, vae1, opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"InvertedPendulum-v2\")\n",
    "env.reset()\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] \n",
    "action_low = env.action_space.low[0]\n",
    "action_high = env.action_space.high[0]\n",
    "state_low = -10.0\n",
    "state_high = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale(x):\n",
    "    # State\n",
    "    ((x[:, 0].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 1].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 2].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 3].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "\n",
    "    # Action\n",
    "    ((x[:, 4].mul_(action_high-action_low)).add_(action_low)).to(device)\n",
    "    \n",
    "    # State\n",
    "    ((x[:, 5].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 6].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 7].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 8].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    \n",
    "    # Reward\n",
    "    (x[:, 9].mul_(20.0)).to(device)\n",
    "    \n",
    "    # Done\n",
    "    (x[:, 10].round_()).to(device)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\n",
      "tensor(0.3830, device='cuda:0')\n",
      "tensor(0.4939, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.2177, device='cuda:0')\n",
      "tensor(0.5038, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.3483, device='cuda:0')\n",
      "tensor(0.5024, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.7430, device='cuda:0')\n",
      "tensor(0.4945, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.8342, device='cuda:0')\n",
      "tensor(0.4850, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.3217, device='cuda:0')\n",
      "tensor(0.4960, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.8454, device='cuda:0')\n",
      "tensor(0.4896, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.7442, device='cuda:0')\n",
      "tensor(0.4998, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.1871, device='cuda:0')\n",
      "tensor(0.4898, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.5779, device='cuda:0')\n",
      "tensor(0.5089, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "print(\"Action\")\n",
    "for i in range(10):\n",
    "    sample = torch.FloatTensor(2, 11).uniform_(0, 1).to(\"cuda\")\n",
    "    print(sample[0][5])\n",
    "    x = vae1(sample)\n",
    "    print(x[0][0][5], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward\n",
      "tensor(0.1024, device='cuda:0')\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.1188, device='cuda:0')\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0548, device='cuda:0')\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0601, device='cuda:0')\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0776, device='cuda:0')\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0331, device='cuda:0')\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0964, device='cuda:0')\n",
      "tensor(0.0725, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0853, device='cuda:0')\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0625, device='cuda:0')\n",
      "tensor(0.0500, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0943, device='cuda:0')\n",
      "tensor(0.0629, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reward\")\n",
    "for i in range(10):\n",
    "    sample = torch.FloatTensor(2, 11).uniform_(0, 0.15).to(\"cuda\")\n",
    "    print(sample[0][9])\n",
    "    x = vae1(sample)\n",
    "    print(x[0][0][9], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:       tensor([0.502, 0.499, 0.518, 0.488, 0.206, 0.502, 0.500, 0.486, 0.563, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.500,     0.501,     0.509,     0.484,     0.196,     0.500,     0.501,     0.473,     0.560,     0.050,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([-1.342,  8.316,  6.731], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 6.243222236633301\n",
      "\n",
      "\n",
      "Original:       tensor([0.499, 0.502, 0.460, 0.593, 0.010, 0.497, 0.508, 0.407, 0.713, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.499,     0.504,     0.461,     0.593,     0.031,     0.496,     0.510,     0.406,     0.707,     0.050,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([-0.555,  8.698,  7.771], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 5.67933988571167\n",
      "\n",
      "\n",
      "Original:       tensor([0.499, 0.501, 0.499, 0.503, 0.328, 0.498, 0.502, 0.480, 0.548, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.500,     0.501,     0.500,     0.503,     0.309,     0.499,     0.502,     0.478,     0.547,     0.050,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([-1.148,  8.605,  6.394], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 6.371553421020508\n",
      "\n",
      "\n",
      "Original:       tensor([0.502, 0.497, 0.542, 0.401, 0.691, 0.504, 0.492, 0.562, 0.355, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.501,     0.498,     0.546,     0.399,     0.731,     0.503,     0.493,     0.564,     0.352,     0.050,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([-1.833,  8.479,  4.801], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 6.29202938079834\n",
      "\n",
      "\n",
      "Original:       tensor([0.499, 0.502, 0.496, 0.509, 0.340, 0.499, 0.503, 0.478, 0.552, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.500,     0.501,     0.497,     0.511,     0.322,     0.499,     0.503,     0.477,     0.551,     0.050,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([-1.101,  8.672,  6.385], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 6.379012107849121\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=3, sci_mode=False, linewidth=140, profile=None)\n",
    "import random\n",
    "for i in range(5):\n",
    "    k = random.randint(1, 400)\n",
    "    sample = test_data[k].to(device)\n",
    "    x, m, s = vae1(sample)\n",
    "    loss = loss_function(x, sample, m, s)\n",
    "    \n",
    "    x = x.to(\"cpu\")\n",
    "    sample = sample.to(\"cpu\")\n",
    "    \n",
    "    print(f\"Original:       {sample}\")\n",
    "    sample = sample.to(device)\n",
    "    print(f\"Reconstruction: {x}\\n\")\n",
    "    torch.set_printoptions(precision=3, sci_mode=False, linewidth=140, profile=\"short\")\n",
    "    print(f\"Latent: {vae1.reparameterize(list(vae1.encode(sample))[0], list(vae1.encode(sample))[1])}\")\n",
    "    print(f\"\\nLoss: {loss}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Variable(torch.rand(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = []\n",
    "\n",
    "for i in range(300):\n",
    "    k = random.randint(1, 6000)\n",
    "    sample = test_data[k].to(device)\n",
    "    l = vae1.reparameterize(list(vae1.encode(sample))[0], list(vae1.encode(sample))[1])\n",
    "    latents.append([i.item() for i in l])\n",
    "    \n",
    "np.save(f\"Latents/latent{C}\", latents)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNBALANCED DATASET\n",
      "counts of intervals: [0, 0.1]: 1965, [0.1, 0.7]: 5781, [0.7, 1]: 2254\n",
      "-- so the ratio to done and not done (deleting the middle ones) is 0.5342498222327566\n",
      "The real ration is 0.17664\n"
     ]
    }
   ],
   "source": [
    "res = [0, 0, 0]\n",
    "k = []\n",
    "for i in range(10000):\n",
    "    sample = Variable(torch.randn(1, LATENT_SIZE))\n",
    "    a = vae1.decode(sample.to(\"cuda\"))[0]\n",
    "    num = float(sum(list(a[10:])))\n",
    "    k.append(num)\n",
    "    if num < 0.1:\n",
    "        res[0] += 1\n",
    "    elif num <0.9:\n",
    "        res[1] += 1\n",
    "    else:\n",
    "        res[2] += 1\n",
    "        \n",
    "d = 0\n",
    "nd = 0\n",
    "\n",
    "for a in test_data:\n",
    "    if a[10] == 1:\n",
    "        d += 1\n",
    "    else:\n",
    "        nd += 1\n",
    "        \n",
    "        \n",
    "print(f\"UNBALANCED DATASET\")\n",
    "print(f\"counts of intervals: [0, 0.1]: {res[0]}, [0.1, 0.7]: {res[1]}, [0.7, 1]: {res[2]}\")\n",
    "print(f\"-- so the ratio to done and not done (deleting the middle ones) is {res[2]/(res[2]+res[0])}\")\n",
    "print(f\"The real ration is {d/(nd+d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.503, 0.494, 0.577, 0.325, 0.500, 0.506, 0.487, 0.576, 0.330, 0.050, 1.000])\n",
      "tensor([0.503, 0.494, 0.578, 0.329, 0.510, 0.506, 0.487, 0.575, 0.329, 0.050, 1.000], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "dat = torch.Tensor([0.503, 0.494, 0.577, 0.325, 0.500, 0.506, 0.487, 0.576, 0.330, 0.050, 1.000])\n",
    "print(dat)\n",
    "x, _, _ = vae1(dat.to(device))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
