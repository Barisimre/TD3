{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 30\n",
    "LAYER_SIZE = 30\n",
    "S_LAYER_SIZE = 15\n",
    "LATENT_SIZE = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 11])\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the file\n",
    "\n",
    "saved = np.load(\"testing.npy\", allow_pickle=True)\n",
    "\n",
    "# Make a trainging and testing batch\n",
    "train_data = torch.Tensor(saved[:int(len(saved)*0.5)])\n",
    "test_data = torch.Tensor(saved[int(len(saved)*0.5):])\n",
    "print(train_data.shape)\n",
    "INPUT_SIZE = len(test_data[0])\n",
    "INPUT_SIZE\n",
    "\n",
    "torch.set_printoptions(linewidth=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.l1 = nn.Linear(INPUT_SIZE, S_LAYER_SIZE)\n",
    "        self.d1 = nn.Linear(S_LAYER_SIZE, LAYER_SIZE)\n",
    "        self.d2 = nn.Linear(LAYER_SIZE, S_LAYER_SIZE)\n",
    "        self.l2a = nn.Linear(S_LAYER_SIZE, LATENT_SIZE)\n",
    "        self.l2b = nn.Linear(S_LAYER_SIZE, LATENT_SIZE)\n",
    "        \n",
    "        # Decoder\n",
    "        self.l3 = nn.Linear(LATENT_SIZE, S_LAYER_SIZE)\n",
    "        self.d3 = nn.Linear(S_LAYER_SIZE, LAYER_SIZE)\n",
    "        self.d4 = nn.Linear(LAYER_SIZE, S_LAYER_SIZE)\n",
    "        self.l4 = nn.Linear(S_LAYER_SIZE, INPUT_SIZE)\n",
    "            \n",
    "    # Run some data through the encoder\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.d1(x))\n",
    "        x = F.relu(self.d2(x))\n",
    "        # return the mu and the sigma\n",
    "        return self.l2a(x), self.l2b(x)\n",
    "    \n",
    "    # The reparameterization trick, taken from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        std = torch.exp(0.5*sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.d3(x))\n",
    "        x = F.relu(self.d4(x))\n",
    "        return torch.sigmoid(self.l4(x)) # sigmoid vs tanh\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: check the shape of x to be sure we have the right input\n",
    "        mu, sigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        # The loss function needs the mu and the sigma so just return them here\n",
    "        return self.decode(z), mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "# Taken from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "def loss_function(recon_x, x, mu, sigma):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "#     KLD = -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())\n",
    "#     return BCE + KLD\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e_count, model, optimizer):\n",
    "    data = train_data\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(0, len(data), BATCH_SIZE):\n",
    "        batch = data[i:i+BATCH_SIZE].to(device)\n",
    "        model.zero_grad()\n",
    "        recons, mu, sigma = model(batch)\n",
    "        loss = loss_function(recons, batch, mu, sigma)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch: {e_count}, Loss: {train_loss/len(data)}\")        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use CPU\n",
    "def test(e_count, model, optimizer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for i in range(0, len(test_data), BATCH_SIZE):\n",
    "        batch = test_data[i:i+BATCH_SIZE].to(device)\n",
    "        recons, mu, sigma = model(batch)\n",
    "        loss = loss_function(recons, batch, mu, sigma)\n",
    "        test_loss += loss.item()\n",
    "    print(f\"TEST Epoch: {e_count}, Loss: {test_loss/len(test_data)}\") \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[torch.randperm(train_data.size()[0])]\n",
    "test_data=test_data[torch.randperm(test_data.size()[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae1 = VAE().to(device)\n",
    "opt1 = optim.Adam(vae1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 6.738183416748047\n",
      "Epoch: 1, Loss: 6.254246844482422\n",
      "Epoch: 2, Loss: 6.2341465502929685\n",
      "Epoch: 3, Loss: 6.23286374633789\n",
      "Epoch: 4, Loss: 6.232385623168946\n",
      "Epoch: 5, Loss: 6.232164191894531\n",
      "Epoch: 6, Loss: 6.232026492919922\n",
      "Epoch: 7, Loss: 6.231876740722656\n",
      "Epoch: 8, Loss: 6.2317273309326175\n",
      "Epoch: 9, Loss: 6.2315262127685545\n",
      "Epoch: 10, Loss: 6.231237077026367\n",
      "Epoch: 11, Loss: 6.230756421508789\n",
      "Epoch: 12, Loss: 6.229849782714844\n",
      "Epoch: 13, Loss: 6.227871381225586\n",
      "Epoch: 14, Loss: 6.22282283203125\n",
      "Epoch: 15, Loss: 6.213751964111328\n",
      "Epoch: 16, Loss: 6.2076814221191405\n",
      "Epoch: 17, Loss: 6.204412852172852\n",
      "Epoch: 18, Loss: 6.203541481933594\n",
      "Epoch: 19, Loss: 6.203331782226562\n",
      "Epoch: 20, Loss: 6.203235104370117\n",
      "Epoch: 21, Loss: 6.2031801879882815\n",
      "Epoch: 22, Loss: 6.203138837890625\n",
      "Epoch: 23, Loss: 6.203102510986328\n",
      "Epoch: 24, Loss: 6.203069411010742\n",
      "Epoch: 25, Loss: 6.203049979248047\n",
      "Epoch: 26, Loss: 6.203027161865235\n",
      "Epoch: 27, Loss: 6.203007424316406\n",
      "Epoch: 28, Loss: 6.202985422363281\n",
      "Epoch: 29, Loss: 6.202972390136718\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    train(i, vae1, opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"InvertedPendulum-v2\")\n",
    "env.reset()\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] \n",
    "action_low = env.action_space.low[0]\n",
    "action_high = env.action_space.high[0]\n",
    "state_low = -10.0\n",
    "state_high = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale(x):\n",
    "    # State\n",
    "    ((x[:, 0].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 1].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 2].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 3].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "\n",
    "    # Action\n",
    "    ((x[:, 4].mul_(action_high-action_low)).add_(action_low)).to(device)\n",
    "    \n",
    "    # State\n",
    "    ((x[:, 5].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 6].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 7].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    ((x[:, 8].mul_(state_high-state_low)).add_(state_low)).to(device)\n",
    "    \n",
    "    # Reward\n",
    "    (x[:, 9].mul_(20.0)).to(device)\n",
    "    \n",
    "    # Done\n",
    "    (x[:, 10].round_()).to(device)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\n",
      "tensor(0.0709, device='cuda:0')\n",
      "tensor(0.5087, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.8194, device='cuda:0')\n",
      "tensor(0.5025, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.8441, device='cuda:0')\n",
      "tensor(0.4926, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.1289, device='cuda:0')\n",
      "tensor(0.5039, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.8498, device='cuda:0')\n",
      "tensor(0.4980, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.1809, device='cuda:0')\n",
      "tensor(0.4919, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.9225, device='cuda:0')\n",
      "tensor(0.4694, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.6081, device='cuda:0')\n",
      "tensor(0.5098, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.4483, device='cuda:0')\n",
      "tensor(0.5017, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.9408, device='cuda:0')\n",
      "tensor(0.5024, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "print(\"Action\")\n",
    "for i in range(10):\n",
    "    sample = torch.FloatTensor(2, 11).uniform_(0, 1).to(\"cuda\")\n",
    "    print(sample[0][5])\n",
    "    x = vae1(sample)\n",
    "    print(x[0][0][5], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward\n",
      "tensor(0.1183, device='cuda:0')\n",
      "tensor(0.0609, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0431, device='cuda:0')\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0486, device='cuda:0')\n",
      "tensor(0.0667, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0923, device='cuda:0')\n",
      "tensor(0.0304, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.1363, device='cuda:0')\n",
      "tensor(0.0447, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0311, device='cuda:0')\n",
      "tensor(0.0639, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.1470, device='cuda:0')\n",
      "tensor(0.0694, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.1196, device='cuda:0')\n",
      "tensor(0.0671, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0429, device='cuda:0')\n",
      "tensor(0.0665, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n",
      "tensor(0.0167, device='cuda:0')\n",
      "tensor(0.0677, device='cuda:0', grad_fn=<SelectBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reward\")\n",
    "for i in range(10):\n",
    "    sample = torch.FloatTensor(2, 11).uniform_(0, 0.15).to(\"cuda\")\n",
    "    print(sample[0][9])\n",
    "    x = vae1(sample)\n",
    "    print(x[0][0][9], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:       tensor([0.501, 0.499, 0.465, 0.576, 0.956, 0.501, 0.500, 0.515, 0.454, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.500,     0.502,     0.464,     0.578,     0.907,     0.500,     0.500,     0.517,     0.457,     0.051,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([ 3.548, 10.166, -6.968, -2.254, -5.846,  6.463], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 5.922659873962402\n",
      "\n",
      "\n",
      "Original:       tensor([0.498, 0.505, 0.541, 0.410, 0.993, 0.501, 0.499, 0.594, 0.296, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.501,     0.499,     0.536,     0.418,     0.921,     0.505,     0.492,     0.591,     0.300,     0.051,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([ 3.524,  9.495, -6.428, -2.631, -5.777,  6.409], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 5.720589637756348\n",
      "\n",
      "\n",
      "Original:       tensor([0.501, 0.497, 0.552, 0.378, 0.031, 0.502, 0.495, 0.500, 0.502, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.503,     0.499,     0.552,     0.380,     0.033,     0.505,     0.493,     0.501,     0.501,     0.048,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([ 1.577,  5.090, -3.938,  1.193, -1.469,  1.992], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 5.84670352935791\n",
      "\n",
      "\n",
      "Original:       tensor([0.498, 0.506, 0.467, 0.591, 0.268, 0.496, 0.511, 0.442, 0.650, 0.050, 1.000])\n",
      "Reconstruction: tensor([0.501, 0.503, 0.462, 0.593, 0.238, 0.500, 0.508, 0.436, 0.647, 0.051, 1.000], grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([-1.349,  1.440, -2.748,  9.342,  5.149, -4.829], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 6.2564897537231445\n",
      "\n",
      "\n",
      "Original:       tensor([0.501, 0.498, 0.480, 0.547, 0.192, 0.499, 0.501, 0.447, 0.620, 0.050, 0.000])\n",
      "Reconstruction: tensor([    0.500,     0.501,     0.477,     0.550,     0.184,     0.499,     0.504,     0.445,     0.624,     0.050,     0.000],\n",
      "       grad_fn=<CopyBackwards>)\n",
      "\n",
      "Latent: tensor([ 2.004,  6.670, -4.993,  0.779, -2.428,  2.960], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "Loss: 6.192648410797119\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=3, sci_mode=False, linewidth=140, profile=None)\n",
    "import random\n",
    "for i in range(5):\n",
    "    k = random.randint(1, 400)\n",
    "    sample = test_data[k].to(device)\n",
    "    x, m, s = vae1(sample)\n",
    "    loss = loss_function(x, sample, m, s)\n",
    "    \n",
    "    x = x.to(\"cpu\")\n",
    "    sample = sample.to(\"cpu\")\n",
    "    \n",
    "    print(f\"Original:       {sample}\")\n",
    "    sample = sample.to(device)\n",
    "    print(f\"Reconstruction: {x}\\n\")\n",
    "    print(f\"Latent: {list(vae1.encode(sample))[0]}\")\n",
    "    print(f\"\\nLoss: {loss}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.502, 0.494, 0.530, 0.425, 0.993, 0.505, 0.489, 0.583, 0.301, 0.050, 1.000])\n",
      "tensor([0.502, 0.497, 0.539, 0.411, 0.140, 0.502, 0.496, 0.499, 0.505, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.001, 0.499, 0.502, 0.445, 0.627, 0.050, 0.000])\n",
      "tensor([0.502, 0.495, 0.515, 0.468, 0.998, 0.504, 0.491, 0.569, 0.339, 0.050, 0.000])\n",
      "tensor([0.503, 0.494, 0.537, 0.424, 0.825, 0.506, 0.489, 0.572, 0.341, 0.050, 1.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.101, 0.499, 0.502, 0.457, 0.602, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.052, 0.499, 0.502, 0.450, 0.614, 0.050, 0.000])\n",
      "tensor([0.501, 0.498, 0.480, 0.550, 0.625, 0.500, 0.499, 0.494, 0.513, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.536, 0.415, 0.027, 0.501, 0.499, 0.483, 0.541, 0.050, 0.000])\n",
      "tensor([0.501, 0.498, 0.569, 0.371, 0.762, 0.504, 0.492, 0.597, 0.311, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.064, 0.499, 0.502, 0.452, 0.611, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.623, 0.500, 0.499, 0.514, 0.468, 0.050, 0.000])\n",
      "tensor([0.498, 0.504, 0.493, 0.516, 0.662, 0.498, 0.504, 0.510, 0.479, 0.050, 0.000])\n",
      "tensor([0.495, 0.506, 0.488, 0.516, 0.376, 0.494, 0.507, 0.474, 0.553, 0.050, 0.000])\n",
      "tensor([0.500, 0.501, 0.477, 0.553, 0.600, 0.499, 0.503, 0.488, 0.526, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.496, 0.508, 0.945, 0.501, 0.498, 0.545, 0.394, 0.050, 0.000])\n",
      "tensor([0.503, 0.494, 0.552, 0.381, 0.821, 0.505, 0.488, 0.586, 0.303, 0.050, 1.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.879, 0.501, 0.498, 0.542, 0.404, 0.050, 0.000])\n",
      "tensor([0.502, 0.502, 0.554, 0.420, 0.708, 0.505, 0.498, 0.576, 0.375, 0.050, 0.000])\n",
      "tensor([0.501, 0.498, 0.551, 0.382, 0.630, 0.503, 0.493, 0.564, 0.355, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.927, 0.501, 0.497, 0.547, 0.391, 0.050, 0.000])\n",
      "tensor([0.500, 0.502, 0.456, 0.604, 0.474, 0.498, 0.506, 0.454, 0.606, 0.050, 0.000])\n",
      "tensor([0.500, 0.498, 0.478, 0.535, 0.536, 0.500, 0.500, 0.482, 0.522, 0.050, 0.000])\n",
      "tensor([0.497, 0.506, 0.510, 0.474, 0.963, 0.499, 0.503, 0.560, 0.365, 0.050, 0.000])\n",
      "tensor([0.499, 0.501, 0.463, 0.584, 0.377, 0.498, 0.505, 0.450, 0.611, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.131, 0.499, 0.501, 0.460, 0.594, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.288, 0.500, 0.501, 0.477, 0.554, 0.050, 0.000])\n",
      "tensor([0.499, 0.501, 0.486, 0.534, 0.102, 0.498, 0.504, 0.442, 0.634, 0.050, 0.000])\n",
      "tensor([0.497, 0.508, 0.482, 0.542, 0.631, 0.496, 0.509, 0.495, 0.516, 0.050, 0.000])\n",
      "tensor([0.499, 0.504, 0.502, 0.510, 0.479, 0.499, 0.505, 0.499, 0.520, 0.050, 0.000])\n",
      "tensor([0.498, 0.506, 0.480, 0.560, 0.054, 0.496, 0.511, 0.431, 0.675, 0.050, 1.000])\n",
      "tensor([0.500, 0.498, 0.545, 0.396, 0.511, 0.502, 0.494, 0.545, 0.398, 0.050, 0.000])\n",
      "tensor([0.496, 0.509, 0.429, 0.660, 0.462, 0.493, 0.515, 0.426, 0.668, 0.050, 1.000])\n",
      "tensor([0.500, 0.502, 0.464, 0.583, 0.721, 0.499, 0.504, 0.489, 0.523, 0.050, 0.000])\n",
      "tensor([0.501, 0.498, 0.566, 0.348, 0.564, 0.504, 0.491, 0.572, 0.339, 0.050, 0.000])\n",
      "tensor([0.500, 0.501, 0.520, 0.454, 0.088, 0.500, 0.501, 0.474, 0.563, 0.050, 0.000])\n",
      "tensor([0.500, 0.501, 0.484, 0.539, 0.839, 0.500, 0.501, 0.521, 0.452, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.935, 0.501, 0.498, 0.548, 0.390, 0.050, 0.000])\n",
      "tensor([0.499, 0.501, 0.479, 0.548, 0.456, 0.498, 0.503, 0.475, 0.557, 0.050, 0.000])\n",
      "tensor([0.497, 0.506, 0.412, 0.688, 0.782, 0.494, 0.512, 0.444, 0.612, 0.050, 1.000])\n",
      "tensor([0.502, 0.498, 0.553, 0.400, 0.531, 0.504, 0.494, 0.555, 0.397, 0.050, 0.000])\n",
      "tensor([0.499, 0.504, 0.492, 0.537, 0.715, 0.499, 0.505, 0.515, 0.486, 0.050, 0.000])\n",
      "tensor([0.498, 0.504, 0.464, 0.582, 0.974, 0.498, 0.505, 0.516, 0.462, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.848, 0.500, 0.498, 0.539, 0.411, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.663, 0.501, 0.500, 0.518, 0.459, 0.050, 0.000])\n",
      "tensor([0.495, 0.501, 0.412, 0.643, 0.894, 0.493, 0.505, 0.457, 0.535, 0.050, 0.000])\n",
      "tensor([0.498, 0.503, 0.441, 0.623, 0.205, 0.495, 0.510, 0.410, 0.693, 0.050, 0.000])\n",
      "tensor([0.501, 0.499, 0.525, 0.442, 0.076, 0.501, 0.499, 0.478, 0.553, 0.050, 0.000])\n",
      "tensor([0.500, 0.499, 0.513, 0.470, 0.777, 0.501, 0.497, 0.543, 0.401, 0.050, 0.000])\n",
      "tensor([0.499, 0.502, 0.455, 0.605, 0.544, 0.497, 0.506, 0.461, 0.589, 0.050, 0.000])\n",
      "tensor([0.500, 0.501, 0.531, 0.431, 0.803, 0.501, 0.497, 0.564, 0.359, 0.050, 0.000])\n",
      "tensor([0.503, 0.497, 0.544, 0.427, 0.487, 0.505, 0.495, 0.542, 0.432, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.501, 0.498, 0.328, 0.500, 0.501, 0.482, 0.542, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.682, 0.501, 0.499, 0.520, 0.453, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.865, 0.501, 0.498, 0.541, 0.407, 0.050, 0.000])\n",
      "tensor([0.499, 0.502, 0.454, 0.606, 0.909, 0.498, 0.504, 0.499, 0.497, 0.050, 0.000])\n",
      "tensor([0.499, 0.501, 0.530, 0.429, 0.485, 0.501, 0.498, 0.528, 0.439, 0.050, 0.000])\n",
      "tensor([0.498, 0.503, 0.480, 0.539, 0.007, 0.496, 0.507, 0.426, 0.665, 0.050, 0.000])\n",
      "tensor([0.504, 0.493, 0.532, 0.447, 0.614, 0.506, 0.490, 0.544, 0.414, 0.050, 1.000])\n",
      "tensor([0.499, 0.501, 0.502, 0.495, 0.503, 0.500, 0.501, 0.502, 0.496, 0.050, 0.000])\n",
      "tensor([0.500, 0.501, 0.473, 0.562, 0.354, 0.498, 0.504, 0.458, 0.597, 0.050, 0.000])\n",
      "tensor([0.501, 0.498, 0.528, 0.435, 0.186, 0.501, 0.497, 0.494, 0.517, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.519, 0.453, 0.382, 0.500, 0.499, 0.506, 0.487, 0.050, 0.000])\n",
      "tensor([0.497, 0.509, 0.463, 0.582, 0.967, 0.496, 0.510, 0.514, 0.471, 0.050, 0.000])\n",
      "tensor([0.501, 0.498, 0.537, 0.414, 0.841, 0.503, 0.493, 0.574, 0.331, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.044, 0.499, 0.502, 0.449, 0.616, 0.050, 0.000])\n",
      "tensor([0.499, 0.502, 0.535, 0.413, 0.987, 0.501, 0.496, 0.588, 0.296, 0.050, 0.000])\n",
      "tensor([0.500, 0.497, 0.506, 0.451, 0.085, 0.499, 0.497, 0.460, 0.556, 0.050, 0.000])\n",
      "tensor([0.500, 0.499, 0.508, 0.481, 0.797, 0.501, 0.497, 0.541, 0.406, 0.050, 0.000])\n",
      "tensor([0.503, 0.493, 0.534, 0.414, 0.953, 0.505, 0.487, 0.583, 0.300, 0.050, 1.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.268, 0.499, 0.501, 0.474, 0.559, 0.050, 0.000])\n",
      "tensor([0.502, 0.496, 0.450, 0.606, 0.453, 0.500, 0.500, 0.446, 0.607, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.499, 0.819, 0.500, 0.498, 0.535, 0.418, 0.050, 0.000])\n",
      "tensor([0.498, 0.504, 0.452, 0.611, 0.603, 0.496, 0.508, 0.464, 0.583, 0.050, 0.000])\n",
      "tensor([0.504, 0.492, 0.495, 0.512, 0.906, 0.505, 0.490, 0.541, 0.400, 0.050, 0.000])\n",
      "tensor([0.497, 0.509, 0.457, 0.612, 0.265, 0.494, 0.515, 0.432, 0.672, 0.050, 1.000])\n",
      "tensor([0.500, 0.500, 0.491, 0.520, 0.921, 0.501, 0.498, 0.537, 0.411, 0.050, 0.000])\n",
      "tensor([0.498, 0.506, 0.452, 0.611, 0.175, 0.495, 0.512, 0.417, 0.691, 0.050, 1.000])\n",
      "tensor([0.507, 0.492, 0.533, 0.456, 0.097, 0.507, 0.492, 0.489, 0.550, 0.050, 0.000])\n",
      "tensor([0.503, 0.493, 0.525, 0.447, 0.479, 0.504, 0.491, 0.523, 0.447, 0.050, 0.000])\n",
      "tensor([0.499, 0.501, 0.478, 0.551, 0.761, 0.499, 0.502, 0.507, 0.483, 0.050, 0.000])\n",
      "tensor([0.497, 0.509, 0.451, 0.620, 0.733, 0.495, 0.512, 0.476, 0.565, 0.050, 1.000])\n",
      "tensor([0.503, 0.494, 0.501, 0.498, 0.008, 0.502, 0.496, 0.447, 0.615, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.034, 0.499, 0.503, 0.449, 0.620, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.270, 0.500, 0.501, 0.475, 0.559, 0.050, 0.000])\n",
      "tensor([0.499, 0.501, 0.499, 0.502, 0.851, 0.500, 0.499, 0.538, 0.413, 0.050, 0.000])\n",
      "tensor([0.501, 0.499, 0.543, 0.401, 0.330, 0.502, 0.496, 0.523, 0.450, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.411, 0.500, 0.500, 0.490, 0.522, 0.050, 0.000])\n",
      "tensor([0.499, 0.502, 0.459, 0.597, 0.623, 0.498, 0.505, 0.473, 0.563, 0.050, 0.000])\n",
      "tensor([0.500, 0.501, 0.470, 0.571, 0.912, 0.500, 0.502, 0.516, 0.463, 0.050, 0.000])\n",
      "tensor([0.503, 0.495, 0.564, 0.366, 0.230, 0.505, 0.491, 0.534, 0.437, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.380, 0.500, 0.500, 0.487, 0.530, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.511, 0.474, 0.633, 0.501, 0.499, 0.525, 0.442, 0.050, 0.000])\n",
      "tensor([0.500, 0.499, 0.527, 0.433, 0.797, 0.502, 0.495, 0.559, 0.361, 0.050, 0.000])\n",
      "tensor([0.501, 0.498, 0.478, 0.554, 0.789, 0.501, 0.498, 0.510, 0.474, 0.050, 0.000])\n",
      "tensor([0.500, 0.500, 0.500, 0.500, 0.648, 0.501, 0.499, 0.516, 0.462, 0.050, 0.000])\n",
      "tensor([0.499, 0.502, 0.464, 0.584, 0.875, 0.498, 0.503, 0.505, 0.486, 0.050, 0.000])\n",
      "tensor([0.504, 0.491, 0.599, 0.273, 0.968, 0.509, 0.480, 0.646, 0.171, 0.050, 1.000])\n",
      "tensor([0.501, 0.499, 0.531, 0.427, 0.573, 0.502, 0.495, 0.539, 0.412, 0.050, 0.000])\n"
     ]
    }
   ],
   "source": [
    "for i in range(99):\n",
    "    print(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0.501,     0.500,     0.497,     0.504,     0.626,     0.501,     0.499,     0.510,     0.477,     0.051,     0.000],\n",
       "        device='cuda:0', grad_fn=<SigmoidBackward>),\n",
       " tensor([ 2.789,  8.215, -5.806, -0.920, -4.158,  4.739], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([-14.816, -16.048, -12.929, -15.694, -13.095, -15.291], device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae1(test_data[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
