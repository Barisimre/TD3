{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "INPUT_SIZE = 11\n",
    "LAYER_SIZE = 10\n",
    "LATENT_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the file\n",
    "\n",
    "saved = np.load(\"testing.npy\", allow_pickle=True)\n",
    "\n",
    "# Make a trainging and testing batch\n",
    "train_data = torch.Tensor(saved[:int(len(saved)*0.1)])\n",
    "test_data = torch.Tensor(saved[int(len(saved)*0.9):])\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.l1 = nn.Linear(INPUT_SIZE, LAYER_SIZE)\n",
    "        self.l2a = nn.Linear(LAYER_SIZE, LATENT_SIZE)\n",
    "        self.l2b = nn.Linear(LAYER_SIZE, LATENT_SIZE)\n",
    "        \n",
    "        # Decoder\n",
    "        self.l3 = nn.Linear(LATENT_SIZE, LAYER_SIZE)\n",
    "        self.l4 = nn.Linear(LAYER_SIZE, INPUT_SIZE)\n",
    "    \n",
    "    # Run some data through the encoder\n",
    "    def encode(self, x):\n",
    "        out = F.relu(self.l1(x))\n",
    "        # return the mu and the sigma\n",
    "        return self.l2a(out), self.l2b(out)\n",
    "    \n",
    "    # The reparameterization trick, taken from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        std = torch.exp(0.5*sigma)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, x):\n",
    "        out = F.relu(self.l3(x))\n",
    "        return torch.sigmoid(self.l4(out)) # sigmoid vs tanh\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: check the shape of x to be sure we have the right input\n",
    "        mu, sigma = self.encode(x)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        # The loss function needs the mu and the sigma so just return them here\n",
    "        return self.decode(z), mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "# Taken from https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "def loss_function(recon_x, x, mu, sigma):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "why = VAE().to(device)\n",
    "optimizer = optim.Adam(why.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e_count):\n",
    "    why.train()\n",
    "    train_loss = 0\n",
    "    for i in range(0, len(train_data), BATCH_SIZE):\n",
    "        batch = train_data[i:i+BATCH_SIZE].to(device)\n",
    "        why.zero_grad()\n",
    "        recons, mu, sigma = why(batch)\n",
    "        loss = loss_function(recons, batch, mu, sigma)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch: {e_count}, Loss: {train_loss/len(train_data)}\")        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use CPU\n",
    "def test(e_count):\n",
    "    why.eval()\n",
    "    test_loss = 0\n",
    "    for i in range(0, len(test_data), BATCH_SIZE):\n",
    "        batch = test_data[i:i+BATCH_SIZE].to(device)\n",
    "        recons, mu, sigma = why(batch)\n",
    "        loss = loss_function(recons, batch, mu, sigma)\n",
    "        test_loss += loss.item()\n",
    "    print(f\"TEST Epoch: {e_count}, Loss: {test_loss/len(test_data)}\") \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 7.333362358093262\n",
      "Epoch: 1, Loss: 7.03670824508667\n",
      "Epoch: 2, Loss: 6.942564561462403\n",
      "Epoch: 3, Loss: 6.9157506980895995\n",
      "Epoch: 4, Loss: 6.90827720336914\n",
      "Epoch: 5, Loss: 6.9063365982055664\n",
      "Epoch: 6, Loss: 6.905120349884033\n",
      "Epoch: 7, Loss: 6.904976037597656\n",
      "Epoch: 8, Loss: 6.904421378326416\n",
      "Epoch: 9, Loss: 6.903751925659179\n",
      "Epoch: 10, Loss: 6.903700095367432\n",
      "Epoch: 11, Loss: 6.903808680725097\n",
      "Epoch: 12, Loss: 6.903047301483154\n",
      "Epoch: 13, Loss: 6.902515979003907\n",
      "Epoch: 14, Loss: 6.902414444732666\n",
      "Epoch: 15, Loss: 6.901397582244873\n",
      "Epoch: 16, Loss: 6.902827927398682\n",
      "Epoch: 17, Loss: 6.902663004302979\n",
      "Epoch: 18, Loss: 6.903002351379395\n",
      "Epoch: 19, Loss: 6.902345877075195\n",
      "Epoch: 20, Loss: 6.901821361541748\n",
      "Epoch: 21, Loss: 6.9018935852050785\n",
      "Epoch: 22, Loss: 6.902493190765381\n",
      "Epoch: 23, Loss: 6.9018340644836425\n",
      "Epoch: 24, Loss: 6.902389503479004\n",
      "Epoch: 25, Loss: 6.90074647064209\n",
      "Epoch: 26, Loss: 6.901525462341309\n",
      "Epoch: 27, Loss: 6.902431446838379\n",
      "Epoch: 28, Loss: 6.901558889770508\n",
      "Epoch: 29, Loss: 6.902390245819092\n",
      "Epoch: 30, Loss: 6.901330647277832\n",
      "Epoch: 31, Loss: 6.90268074798584\n",
      "Epoch: 32, Loss: 6.901656125640869\n",
      "Epoch: 33, Loss: 6.901847438049316\n",
      "Epoch: 34, Loss: 6.901589315032959\n",
      "Epoch: 35, Loss: 6.90157763671875\n",
      "Epoch: 36, Loss: 6.901613265991211\n",
      "Epoch: 37, Loss: 6.902218918609619\n",
      "Epoch: 38, Loss: 6.902171768951416\n",
      "Epoch: 39, Loss: 6.901412910461426\n",
      "Epoch: 40, Loss: 6.902455767822266\n",
      "Epoch: 41, Loss: 6.901930076599121\n",
      "Epoch: 42, Loss: 6.902361756896973\n",
      "Epoch: 43, Loss: 6.9013109840393065\n",
      "Epoch: 44, Loss: 6.902113232421875\n",
      "Epoch: 45, Loss: 6.901142088317871\n",
      "Epoch: 46, Loss: 6.9015587692260745\n",
      "Epoch: 47, Loss: 6.901423587799072\n",
      "Epoch: 48, Loss: 6.901022105407715\n",
      "Epoch: 49, Loss: 6.901942286682129\n",
      "TEST Epoch: 0, Loss: 6.894111627960205\n",
      "TEST Epoch: 1, Loss: 6.894254582977295\n",
      "TEST Epoch: 2, Loss: 6.893268572998047\n",
      "TEST Epoch: 3, Loss: 6.893955154418945\n",
      "TEST Epoch: 4, Loss: 6.893876079559326\n",
      "TEST Epoch: 5, Loss: 6.894798441314697\n",
      "TEST Epoch: 6, Loss: 6.893918427276612\n",
      "TEST Epoch: 7, Loss: 6.892873741912842\n",
      "TEST Epoch: 8, Loss: 6.894670262908935\n",
      "TEST Epoch: 9, Loss: 6.895104915618896\n",
      "TEST Epoch: 10, Loss: 6.894793329620361\n",
      "TEST Epoch: 11, Loss: 6.893787404632568\n",
      "TEST Epoch: 12, Loss: 6.894080763244629\n",
      "TEST Epoch: 13, Loss: 6.8956715209960935\n",
      "TEST Epoch: 14, Loss: 6.894924937438965\n",
      "TEST Epoch: 15, Loss: 6.894611991119385\n",
      "TEST Epoch: 16, Loss: 6.894184155273438\n",
      "TEST Epoch: 17, Loss: 6.89481323928833\n",
      "TEST Epoch: 18, Loss: 6.894540671539307\n",
      "TEST Epoch: 19, Loss: 6.894549234771729\n",
      "TEST Epoch: 20, Loss: 6.894743161010743\n",
      "TEST Epoch: 21, Loss: 6.894023221588135\n",
      "TEST Epoch: 22, Loss: 6.893909565734863\n",
      "TEST Epoch: 23, Loss: 6.893309677886963\n",
      "TEST Epoch: 24, Loss: 6.894098692321777\n",
      "TEST Epoch: 25, Loss: 6.893895533752441\n",
      "TEST Epoch: 26, Loss: 6.894217805480957\n",
      "TEST Epoch: 27, Loss: 6.894046704101562\n",
      "TEST Epoch: 28, Loss: 6.893837673187256\n",
      "TEST Epoch: 29, Loss: 6.89460322341919\n",
      "TEST Epoch: 30, Loss: 6.89376217956543\n",
      "TEST Epoch: 31, Loss: 6.894081575775147\n",
      "TEST Epoch: 32, Loss: 6.894219376373291\n",
      "TEST Epoch: 33, Loss: 6.893741975402832\n",
      "TEST Epoch: 34, Loss: 6.893979067230225\n",
      "TEST Epoch: 35, Loss: 6.894463428497314\n",
      "TEST Epoch: 36, Loss: 6.894691020202637\n",
      "TEST Epoch: 37, Loss: 6.8944599609375\n",
      "TEST Epoch: 38, Loss: 6.895311811065674\n",
      "TEST Epoch: 39, Loss: 6.894093433380127\n",
      "TEST Epoch: 40, Loss: 6.893750373077393\n",
      "TEST Epoch: 41, Loss: 6.893153594207764\n",
      "TEST Epoch: 42, Loss: 6.895590787506103\n",
      "TEST Epoch: 43, Loss: 6.893687047576904\n",
      "TEST Epoch: 44, Loss: 6.894538262939453\n",
      "TEST Epoch: 45, Loss: 6.8953493827819825\n",
      "TEST Epoch: 46, Loss: 6.894560782623291\n",
      "TEST Epoch: 47, Loss: 6.894161751556396\n",
      "TEST Epoch: 48, Loss: 6.892992708587647\n",
      "TEST Epoch: 49, Loss: 6.893823622131348\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    train(i)\n",
    "for i in range(EPOCHS):\n",
    "    test(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    train(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
